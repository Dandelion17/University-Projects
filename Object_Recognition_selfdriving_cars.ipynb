{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RgPMFXuvLpRd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b1a990a7a5a4fd9a2dc5c33f385d6c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d62864b67a8f4aa3b9a7b5e35c001d7b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7c9b4b3862774723ba17f46795876d25",
              "IPY_MODEL_c521f238817f4aa49683eecf9707a3c1",
              "IPY_MODEL_784f58fb210a41a2b5fd16a701c912d1"
            ]
          }
        },
        "d62864b67a8f4aa3b9a7b5e35c001d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7c9b4b3862774723ba17f46795876d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8fd62d0e0f774527ab8615092ab13142",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_59a7e719d86141feaeca4fb2e8ae671a"
          }
        },
        "c521f238817f4aa49683eecf9707a3c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9c178fe9fcc346a6b534671461da6d46",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 167502836,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 167502836,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4121e099acab478d88cb41441a122d11"
          }
        },
        "784f58fb210a41a2b5fd16a701c912d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e1dd9fdc97e4457c8480aa4bf0b3548c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160M/160M [00:03&lt;00:00, 45.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2988442d47664209a6b8aba8b5f7e97a"
          }
        },
        "8fd62d0e0f774527ab8615092ab13142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "59a7e719d86141feaeca4fb2e8ae671a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c178fe9fcc346a6b534671461da6d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4121e099acab478d88cb41441a122d11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e1dd9fdc97e4457c8480aa4bf0b3548c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2988442d47664209a6b8aba8b5f7e97a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  imports\n",
        "\n"
      ],
      "metadata": {
        "id": "E0vnaIAqd6FP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34BbF_ZFtq79"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.models.detection._utils as det_utils\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms as T\n",
        "\n",
        "#important label\n",
        "custom = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDmZv_JvjXdm",
        "outputId": "96daf552-1fdd-4ae6-ec39-8ca66a9ae434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppj8C2fSYMKU"
      },
      "source": [
        "#loading a small set just for testing\n",
        "#also installs devkits\n",
        "\n",
        "#!mkdir -p /data/sets/nuimages  # Make the directory to store the nuImages dataset in.\n",
        "\n",
        "#!wget https://www.nuscenes.org/data/nuimages-v1.0-mini.tgz  # Download the nuImages mini split.\n",
        "\n",
        "#!tar -xf nuimages-v1.0-mini.tgz -C /data/sets/nuimages  # Uncompress the nuImages mini split.\n",
        "\n",
        "!pip install nuscenes-devkit &> /dev/null  # Install nuImages."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssWBMUpBmGO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51ebc4dc-c932-4fe6-9467-6adc0770bdcc"
      },
      "source": [
        "!pip install torchfunc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchfunc\n",
            "  Downloading torchfunc-0.2.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from torchfunc) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->torchfunc) (3.10.0.2)\n",
            "Installing collected packages: torchfunc\n",
            "Successfully installed torchfunc-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from nuimages import NuImages\n",
        "\n",
        "nuim = NuImages(version=\"v1.0-train\", dataroot=\"/content/drive/MyDrive/AML_Project_2021/Metadata/\")"
      ],
      "metadata": {
        "id": "_OJSvRhxLm8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# exploring the dataset "
      ],
      "metadata": {
        "id": "RgPMFXuvLpRd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKf5TK1GaNvs"
      },
      "source": [
        "\n",
        "#nuim = NuImages(dataroot='/data/sets/nuimages', version='v1.0-mini', verbose=False, lazy=True)\n",
        "nuim = NuImages(version=\"v1.0-train\", dataroot=\"/content/drive/MyDrive/AML_Project_2021/Metadata/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nuim = NuImages(version=\"v1.0-val\", dataroot=\"/content/drive/MyDrive/AML_Project_2021/Metadata/\")"
      ],
      "metadata": {
        "id": "DKWv8_ntME7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i3kuZXnVMIwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(nuim.sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcvkZlm1lE9G",
        "outputId": "7e76b545-5aec-438d-b725-4d6e85b0f8d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67279"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3He_XL9UaRKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c156b8-58a3-4897-895d-6ee8838b1507"
      },
      "source": [
        "#data are stored as relational database\n",
        "#each tables contains a list which has a dictionary as element\n",
        "print(\"number of category\",len(nuim.category))\n",
        "print(\"example of one category\", nuim.category[0]  )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of category 25\n",
            "example of one category {'token': '63a94dfa99bb47529567cd90d3b58384', 'name': 'animal', 'description': 'All animals, e.g. cats, rats, dogs, deer, birds.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYtFzFOfbZaH"
      },
      "source": [
        "#TODO insert schema from drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XhBcvrsceCA"
      },
      "source": [
        "info needed for training\n",
        "\n",
        "image: a PIL Image of size (H, W)\n",
        "\n",
        "target: a dict containing the following fields\n",
        "\n",
        "boxes (FloatTensor[N, 4]): the coordinates of the N bounding boxes in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H\n",
        "\n",
        "labels (Int64Tensor[N]): the label for each bounding box. 0 represents always the background class.\n",
        "\n",
        "image_id (Int64Tensor[1]): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
        "\n",
        "area (Tensor[N]): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
        "\n",
        "iscrowd (UInt8Tensor[N]): instances with iscrowd=True will be ignored during evaluation.\n",
        "\n",
        "(optionally) masks (UInt8Tensor[N, H, W]): The segmentation masks for each one of the objects\n",
        "\n",
        "(optionally) keypoints (FloatTensor[N, K, 3]): For each one of the N objects, it contains the K keypoints in [x, y, visibility] format, defining the object. \n",
        "visibility=0 means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt references/detection/transforms.py for your new keypoint representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ymFNY5Jaufs",
        "outputId": "09fc2add-4206-45a1-e70c-c6322b09a589"
      },
      "source": [
        "#listing all tables of the dataset\n",
        "nuim.table_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['attribute',\n",
              " 'calibrated_sensor',\n",
              " 'category',\n",
              " 'ego_pose',\n",
              " 'log',\n",
              " 'object_ann',\n",
              " 'sample',\n",
              " 'sample_data',\n",
              " 'sensor',\n",
              " 'surface_ann']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUyb5RXKdlVL",
        "outputId": "b44c6152-537f-4fc8-f362-cabd0fe6179b"
      },
      "source": [
        "#the sample can be indexed as a simple python list\n",
        "sample_idx = 1\n",
        "sample = nuim.sample[sample_idx]\n",
        "sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'key_camera_token': '7e19c8ab6b184b4baa82a0bb4340bdad',\n",
              " 'log_token': 'a16c68f6c9bd47efaec2d05206dd0637',\n",
              " 'timestamp': 1527709308919956,\n",
              " 'token': '0001746b484f4a0f939ff8c8b836a7e4'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lid6diceapm",
        "outputId": "5921113d-bbeb-4acf-e39c-fccae205724c"
      },
      "source": [
        "#the foreign key from sample to sample data is key camera \n",
        "camera_token = sample['key_camera_token']\n",
        "sample_data = nuim.get(table_name='sample_data', token= camera_token)\n",
        "sample_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'calibrated_sensor_token': '25917959e82d56b1a896fea7574f9419',\n",
              " 'ego_pose_token': '6e501ea92d6c460c9f562bc8b6d0e15d',\n",
              " 'fileformat': 'jpg',\n",
              " 'filename': 'samples/CAM_FRONT_RIGHT/n008-2018-05-30-15-20-59-0400__CAM_FRONT_RIGHT__1527709308919956.jpg',\n",
              " 'height': 900,\n",
              " 'is_key_frame': True,\n",
              " 'next': '37bae0569c1844a0b04affdbba0a8362',\n",
              " 'prev': 'd3d1c91e2be845bab3394afd30d1226c',\n",
              " 'sample_token': '0001746b484f4a0f939ff8c8b836a7e4',\n",
              " 'timestamp': 1527709308919956,\n",
              " 'token': '7e19c8ab6b184b4baa82a0bb4340bdad',\n",
              " 'width': 1600}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSsTU-9ZicrR"
      },
      "source": [
        "\"\"\"\n",
        "nuim.render_image(camera_token, annotation_type='all',\n",
        "                  with_category=True, with_attributes=True, box_line_width=-1, render_scale=3)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOad1jRViq-u",
        "outputId": "19930688-e8c9-4a36-b835-4e616702f66a"
      },
      "source": [
        "object_tokens, surface_tokens = nuim.list_anns(sample['token'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing object annotations:\n",
            "85463f57d63748c1aa2075800eb41544 vehicle.bicycle ['cycle.without_rider']\n",
            "d8de6f716a754d3aae803974f08adea5 movable_object.barrier []\n",
            "\n",
            "Printing surface annotations:\n",
            "2d83247e19c35e5bbfcbd54a40e0c244 flat.driveable_surface\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUUwOK3Lx70V",
        "outputId": "5dd3c56b-47c1-4282-dc71-1c6b1ab4d0c9"
      },
      "source": [
        "#bbox is [xmin, ymin, xmax, ymax]\n",
        "sample_object = nuim.get(table_name=\"object_ann\", token=object_tokens[0] )\n",
        "\n",
        "sample_object\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attribute_tokens': ['725f0121878f4c86909f256fd9df8569'],\n",
              " 'bbox': [1496, 503, 1600, 628],\n",
              " 'category_token': 'fc95c87b806f48f8a1faea2dcc2222a4',\n",
              " 'mask': {'counts': 'ZVVgWjE4bGswMFdUTzNbazBNZVRPPjBLXGowSGNVTzsybjBbaTBnTmRWT2AybGgwYE1UV09gMmxoMGBNVFdPYTJraDBfTVZXT2EyaWgwYE1WV09gMmpoMD5PMDAwMU8wMDAxTzAwME8xTzJOMU8xMGJNXFdPXTFjaDBjTl5XT1wxYmgwZk5cV09aMWRoMGZOXFdPWjFoaDBiTlhXT14xaWgwYk5WV09dMWtoMGNOVVdPXTFraDBjTlVXT10xbGgwbE1WV089TmcxbGgwbE1YV09mMmhoMFpNWVdPOktpMW1oMG5NWFdPOEtqMW1oMGtNVFdPTTU9S2oxbGgwbE1UV09ONjpKbDFsaDBsTVVXT042NktQMmtoMGtNVVdPT2AwVTJdaDBqTVRXTzM+UTJdaTBPME8yTzBPMk8xTjJOMTAxTjJOMk4zTTJNM000TFleTw==',\n",
              "  'size': [900, 1600]},\n",
              " 'sample_data_token': '7e19c8ab6b184b4baa82a0bb4340bdad',\n",
              " 'token': '85463f57d63748c1aa2075800eb41544'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9OHOuLD1ig-",
        "outputId": "ed8b4ac1-cb07-4a3f-cff1-676e9a861896"
      },
      "source": [
        "category = nuim.get(table_name='category', token= sample_object['category_token'])\n",
        "print(category)\n",
        "print(\"category number\", nuim.category.index(category))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'token': 'fc95c87b806f48f8a1faea2dcc2222a4', 'name': 'vehicle.bicycle', 'description': 'Human or electric powered 2-wheeled vehicle designed to travel at lower speeds either on road surface, sidewalks or bike paths.'}\n",
            "category number 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dataloader"
      ],
      "metadata": {
        "id": "cioYdp7nLyqE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmlgkzgghD8d"
      },
      "source": [
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xRjzcpNjBta"
      },
      "source": [
        "class PennFudanDataset(torch.utils.data.Dataset): \n",
        "  def __init__(self , root, transforms,  version='v1.0-train', verbose=False, lazy=True ):\n",
        "    # version=None, verbose=True, lazy = True, root, transforms=None, version=None, verbose=True, lazy = True\n",
        "    #super.__init__()\n",
        "    self.root = root\n",
        "    self.nuim = NuImages(dataroot=root, version=version, verbose=verbose, lazy=lazy)\n",
        "    print(\"dataset loaded using version:\",version)\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    #get sample at index idx\n",
        "    sample = self.nuim.sample[idx]\n",
        "    #get camera token to retrive and load image information\n",
        "    camera_token = sample['key_camera_token']\n",
        "    sample_data = self.nuim.get(table_name='sample_data', token= camera_token)\n",
        "    image_path_rel = sample_data['filename']\n",
        "    image_path_absolute = os.path.join(self.root, image_path_rel)\n",
        "    img = Image.open(image_path_absolute).convert('RGB')\n",
        "\n",
        "    #normalization like in imagenet\n",
        "    img_tensor = T.ToTensor()(img)\n",
        "    img = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img_tensor)\n",
        "    \n",
        "    object_tokens, surface_tokens = self.nuim.list_anns(sample['token'])\n",
        "\n",
        "    classes = []\n",
        "    boxes = []\n",
        "\n",
        "    for obj_t in object_tokens:\n",
        "      sample_object = self.nuim.get(table_name=\"object_ann\", token=obj_t )\n",
        "      box = sample_object['bbox'] #torch.as_tensor( , dtype=torch.float32\n",
        "      #Skip the guilty box\n",
        "      if (box[0] >= box[2]) or (box[1]>= box[3]) or (box[0]< 0 or box[1] <0 or box[2] < 0 or box[3] < 0):\n",
        "        continue\n",
        "\n",
        "      boxes.append(box)\n",
        "      category = self.nuim.get(table_name='category', token= sample_object['category_token'])\n",
        "      #class is +1 since category 0 is background\n",
        "      obj_class = self.nuim.category.index(category)  + 1\n",
        "      classes.append(obj_class)\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes \n",
        "    target[\"labels\"] = torch.as_tensor(classes, dtype=torch.int64)\n",
        "    #target[\"masks\"] = masks\n",
        "    target[\"image_id\"] = torch.tensor([idx])\n",
        "    #target[\"area\"] = area\n",
        "    #target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "    return img_tensor, target #ex img\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.nuim.sample)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "-QE0uH6wWqb9",
        "outputId": "fff5e1b9-b41f-4306-bffd-4c0937bf46f9"
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_dataset = PennFudanDataset( \"/content/drive/MyDrive/AML_Project_2021/Metadata/\", None, version=\"v1.0-train\")\n",
        "\n",
        "\n",
        "\n",
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_dataset = PennFudanDataset( \"/content/drive/MyDrive/AML_Project_2021/Metadata/\", None, version=\"v1.0-val\")\n",
        "val_data_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "'''\n",
        "valid_data_loader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset loaded using version: v1.0-train\n",
            "dataset loaded using version: v1.0-val\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nvalid_data_loader = DataLoader(\\n    valid_dataset,\\n    batch_size=8,\\n    shuffle=False,\\n    num_workers=4,\\n    collate_fn=collate_fn\\n)'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preparing the model"
      ],
      "metadata": {
        "id": "DYfPa8_kL2i4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgnQ0bIXV_Hx"
      },
      "source": [
        "#just a usefull thing\n",
        "class Averager:\n",
        "    def __init__(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n",
        "\n",
        "    def send(self, value):\n",
        "        self.current_total += value\n",
        "        self.iterations += 1\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        if self.iterations == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1.0 * self.current_total / self.iterations\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2gqNRi7Ubr1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "0b1a990a7a5a4fd9a2dc5c33f385d6c6",
            "d62864b67a8f4aa3b9a7b5e35c001d7b",
            "7c9b4b3862774723ba17f46795876d25",
            "c521f238817f4aa49683eecf9707a3c1",
            "784f58fb210a41a2b5fd16a701c912d1",
            "8fd62d0e0f774527ab8615092ab13142",
            "59a7e719d86141feaeca4fb2e8ae671a",
            "9c178fe9fcc346a6b534671461da6d46",
            "4121e099acab478d88cb41441a122d11",
            "e1dd9fdc97e4457c8480aa4bf0b3548c",
            "2988442d47664209a6b8aba8b5f7e97a"
          ]
        },
        "outputId": "48053f45-3f05-4c3d-eb45-58afbdc9659a"
      },
      "source": [
        "#load a coco pretrained model\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "#se troppo pesante usare mobile net\n",
        "#model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b1a990a7a5a4fd9a2dc5c33f385d6c6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in model.parameters():\n",
        "  i.requires_grad=False"
      ],
      "metadata": {
        "id": "hGQzd4PeL0wV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if custom:\n",
        "  #little hack\n",
        "  model.roi_heads.box_head = torch.nn.Identity()"
      ],
      "metadata": {
        "id": "maIqdBLQuGRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NbzaWEwl5zj",
        "outputId": "d02ae6df-8455-4e12-f787-6942aa51c921"
      },
      "source": [
        "import torchfunc\n",
        "\n",
        "# Assuming model is loaded\n",
        "print(torchfunc.sizeof(model)) #167.446.104 for resnet50fpn "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "111863384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aby5j5jQUgWA"
      },
      "source": [
        "num_classes = 26  # TODO: count categories of nuimages. they should be 27 + background\n",
        "\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAGlmlfkVhfx"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, activation='relu'):\n",
        "     super().__init__()\n",
        "     #with a 3,3 kernel and 1 padding 1 stride there is the same size\n",
        "     self.conv1 = torch.nn.Conv2d(in_channels= in_channels, out_channels= in_channels, kernel_size=(3,3), stride=1, padding=1 )\n",
        "     self.bn1 = torch.nn.BatchNorm2d(in_channels)\n",
        "     #keep the same size with kernel 1,1, but we have upsampled for more output channels\n",
        "     self.conv2 = torch.nn.Conv2d(in_channels= in_channels, out_channels= out_channels, kernel_size=(1,1), stride=1, padding=0 )\n",
        "     self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
        "     #convolution for upsampling the residuals\n",
        "     self.conv1_res = torch.nn.Conv2d(in_channels= in_channels, out_channels= out_channels, kernel_size=(1,1), stride=1, padding=0 )\n",
        "     self.bn1_res = torch.nn.BatchNorm2d(out_channels)\n",
        "     \n",
        "  def forward(self, x):\n",
        "      residual = x\n",
        "\n",
        "      x = self.conv1(x)\n",
        "      x = self.bn1(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.bn2(x)\n",
        "\n",
        "      residual = self.conv1_res(residual)\n",
        "      residual = self.bn1_res(residual)\n",
        "      out = residual + x \n",
        "      out = F.relu(out)\n",
        "      return out"
      ],
      "metadata": {
        "id": "1FSNLCi8Hvve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BottleNeckBlock(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, activation='relu', middle_channel=256):\n",
        "     super().__init__()\n",
        "     #downsampling conv     \n",
        "     self.conv3 = torch.nn.Conv2d(in_channels=in_channels, out_channels=middle_channel, kernel_size=1, padding=0, stride=1)\n",
        "     #3x3 filter\n",
        "     self.conv4 = torch.nn.Conv2d(in_channels=middle_channel, out_channels=middle_channel, kernel_size=(3,3), padding=1, stride=1)\n",
        "     #upsampling\n",
        "     self.conv5 = torch.nn.Conv2d(in_channels=middle_channel, out_channels=out_channels, kernel_size=(1,1), padding=0, stride=1)\n",
        "     self.bn3 = torch.nn.BatchNorm2d(middle_channel)\n",
        "     self.bn4 = torch.nn.BatchNorm2d(middle_channel)\n",
        "     self.bn5 = torch.nn.BatchNorm2d(out_channels)\n",
        "     \n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.bn5(x)\n",
        "\n",
        "    out = F.relu(x) \n",
        "    return out   \n"
      ],
      "metadata": {
        "id": "ohqDaAw4Pbq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NonLocalBlock(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, activation='relu', middle_channel = 512):\n",
        "     super().__init__()\n",
        "     self.mid = middle_channel\n",
        "     self.conv_phy =  torch.nn.Conv2d(in_channels=in_channels, out_channels=middle_channel, kernel_size=1, padding=0, stride=1)\n",
        "     self.conv_theta = torch.nn.Conv2d(in_channels=in_channels, out_channels=middle_channel, kernel_size=1, padding=0, stride=1)\n",
        "     self.conv_g = torch.nn.Conv2d(in_channels=in_channels, out_channels=middle_channel, kernel_size=1, padding=0, stride=1)\n",
        "     self.final_conv = torch.nn.Conv2d(in_channels=middle_channel, out_channels=out_channels, kernel_size=1, padding=0, stride=1)\n",
        "\n",
        "     self.bn_phy = torch.nn.BatchNorm2d(middle_channel)\n",
        "     self.bn_theta = torch.nn.BatchNorm2d(middle_channel)\n",
        "     self.bn_g = torch.nn.BatchNorm2d(middle_channel)\n",
        "     self.bn_fin = torch.nn.BatchNorm2d(out_channels)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "      \n",
        "      residual = x\n",
        "      #we suppose the shape of x is 1024 x 1024 x H x W\n",
        "      #where H and W usually are 7\n",
        "      print(\"the shape of x is ---------------------------------------------------------------------------------:\",x.shape)\n",
        "\n",
        "      #those three shapes are now middle_channel x H x W\n",
        "      theta = self.conv_theta(x)\n",
        "      theta = self.bn_theta(theta)\n",
        "      phy = self.conv_phy(x)\n",
        "      phy = self.bn_phy(phy)\n",
        "      g = self.conv_g(x)\n",
        "      g = self.bn_g(g)\n",
        "\n",
        "      middle_shape = theta.shape #psy and g have equal shapes\n",
        "      #assigning new shape BatchSize x middle_channel x HW\n",
        "      theta_new = theta.view(middle_shape[0], self.mid ,middle_shape[2] * middle_shape[3])\n",
        "      psy_new = phy.view(middle_shape[0], self.mid ,middle_shape[2] * middle_shape[3])\n",
        "      g_new = g.view(middle_shape[0], self.mid ,middle_shape[2] * middle_shape[3])\n",
        "\n",
        "      #g and theta must change to shape Batch x HW x middle_channel\n",
        "      g_new = g_new.permute(0, 2, 1)\n",
        "      theta_new = theta_new.permute(0, 2, 1)\n",
        "\n",
        "      #now the multiplication between theta: (HW x mid) and (mid x HW) results in Batch(?) HW x HW\n",
        "      mat1 = torch.matmul(theta_new, psy_new)\n",
        "      print(\"the shape of mat1 is ---------------------------------------------------------------------------------:\",mat1.shape)\n",
        "      mat1 = F.softmax(mat1, dim=-1)  \n",
        "      # mat2 will have shape Batch x HW x mid\n",
        "      mat2 = torch.matmul(mat1, g_new)\n",
        "\n",
        "      #it must return to the middle shape mid x H x W\n",
        "      #optimization from https://github.com/AlexHex7/Non-local_pytorch/blob/39ad90c91538d34e88865c9fb0ce4a844751346c/Non-Local_pytorch_0.3.1/lib/non_local_embedded_gaussian.py#L68\n",
        "      mat2 = mat2.permute(0, 2, 1).contiguous() \n",
        "      print(\"the shape of mat2 is ---------------------------------------------------------------------------------:\",mat2.shape)\n",
        "      mat2 = mat2.view( middle_shape[0], middle_shape[1], middle_shape[2], middle_shape[3])\n",
        "\n",
        "      #upsample to the output channel size and add residuals\n",
        "      out = self.final_conv(mat2)\n",
        "      out = self.bn_fin(out)\n",
        "      out = out + residual\n",
        "      out = F.relu(out)\n",
        "      print(\"the shape of out is ---------------------------------------------------------------------------------:\",out.shape)\n",
        "      return out\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "ly-h7PNRTJrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#custom predictor\n",
        "class Custom_predictor(torch.nn.Module):\n",
        "    def __init__(self,in_channels,num_classes):\n",
        "        super(Custom_predictor,self).__init__()\n",
        "\n",
        "        #fc_head \n",
        "        self.fc1 = torch.nn.Linear(12544, 1024)\n",
        "        self.cls_score = torch.nn.Linear(1024, num_classes)\n",
        "\n",
        "        #conv head, there may be a combinations of res, bottle and non local block as described in the paper\n",
        "        #residual block upscales to 1024\n",
        "        self.res_block = ResidualBlock(in_channels= 256, out_channels= 1024)\n",
        "        #bottleneck block performs a 3x3 filter on a smaller scale\n",
        "        self.Bneck = BottleNeckBlock(in_channels=1024 , out_channels=1024, middle_channel=256)\n",
        "        #non local box improve foreground\n",
        "        self.non_loc = NonLocalBlock(in_channels=1024, out_channels=1024, middle_channel=512)\n",
        "        self.fc_conv = torch.nn.Linear(1024,num_classes*4)\n",
        "\n",
        "        \"\"\" \n",
        "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
        "        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "        \n",
        "    def forward(self,x):\n",
        "        print(\"shape of x is \", x.shape)\n",
        "\n",
        "        x_conv = x\n",
        "\n",
        "        #fc\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        scores = self.cls_score(x)\n",
        "        \n",
        "        #conv\n",
        "        y = self.res_block(x_conv)        \n",
        "        y = self.non_loc(y)\n",
        "        y = self.Bneck(y)\n",
        "        y = F.avg_pool3d(y , kernel_size=(4,3,3))\n",
        "        y = y.flatten(start_dim=1)\n",
        "        bbox_deltas = self.fc_conv(y)\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        x = x.flatten(start_dim=1)\n",
        "        scores = self.cls_score(x)\n",
        "        bbox_deltas = self.bbox_pred(x)\n",
        "        \"\"\"\n",
        "        return scores, bbox_deltas"
      ],
      "metadata": {
        "id": "2RSl8gmeWFpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if custom:\n",
        "  #assigning the new custom head predictor, our main controbution\n",
        "  model.roi_heads.box_predictor = Custom_predictor(in_features,num_classes)\n",
        "  print(\"we use the double head architecture\")\n",
        "\n",
        "\n",
        "else :\n",
        "  # replace the pre-trained head with a not trained one\n",
        "  #this is the generic faster RCNN predictor\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "  print(\"we use the default predictor with the head not pre-trained\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Diqg8mSgW4jx",
        "outputId": "137395ed-55f1-4785-a27f-08b21a35a116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we use the double head architecture\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in model.rpn.parameters():\n",
        "  i.requires_grad=True\n",
        "for i in model.roi_heads.parameters():\n",
        "  i.requires_grad=True"
      ],
      "metadata": {
        "id": "7As9rICNNqLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for name,i in model.named_parameters():\n",
        "  print(name,i.requires_grad)\n",
        "print(\"number of parameters\",torchfunc.sizeof(model)) \n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcBYrNi8Ncbl",
        "outputId": "a29d95cd-a565-4b7f-861f-43f3096bbfd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfor name,i in model.named_parameters():\\n  print(name,i.requires_grad)\\nprint(\"number of parameters\",torchfunc.sizeof(model)) \\n'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.transform.max_size = 1600"
      ],
      "metadata": {
        "id": "hzbr1ZvhREsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "lr_scheduler = None\n",
        "\n",
        "num_epochs = 2"
      ],
      "metadata": {
        "id": "-MABPooqdNGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "itr = 0\n",
        "\"\"\"\n",
        "for images, targets in train_data_loader:\n",
        "  print(itr)\n",
        "  itr+=1\n",
        "\"\"\"\n",
        "num_epochs=10"
      ],
      "metadata": {
        "id": "R-x_QHmZKf3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "continue_training = True\n",
        "last_itr = 8250"
      ],
      "metadata": {
        "id": "FPxhK6cH_ad7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if continue_training:\n",
        "  if custom:\n",
        "    model.load_state_dict( torch.load( \"/content/drive/MyDrive/AML_Project_2021/Modelli/custom\"+str(last_itr)+\".pth\")) #if on CPU, add this: CPU map_location=torch.device('cpu')\n",
        "  else:\n",
        "    model.load_state_dict( torch.load( \"/content/drive/MyDrive/AML_Project_2021/Modelli/base\"+str(last_itr)+\".pth\")) #if on CPU, add this: CPU map_location=torch.device('cpu')"
      ],
      "metadata": {
        "id": "Cyd0d-4h_nJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.nuim.version  #just to be sure"
      ],
      "metadata": {
        "id": "fEQ_h7WGGCUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf17b3d-847f-47e9-9649-cf709291a623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'v1.0-train'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training"
      ],
      "metadata": {
        "id": "fFxdb50fywlN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb_9Ot9uVodd"
      },
      "source": [
        "\n",
        "loss_hist = Averager()\n",
        "itr = 1\n",
        "for epoch in range(num_epochs):\n",
        "    break #TODO REMOVE\n",
        "    loss_hist.reset()\n",
        "    \n",
        "    for images, targets in train_data_loader:\n",
        "\n",
        "      #sample_data = nuim.get(table_name='sample_data', token= camera_token)\n",
        "      #  print(images, targets, image_ids, x)\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        skip=False\n",
        "        for t in range(len(targets)):\n",
        "          if targets[t]['boxes'].shape == torch.Size([0]):\n",
        "            skip=True\n",
        "            break  #no box just background\n",
        "        if skip or len(targets) == 0:\n",
        "            continue\n",
        "\n",
        "        \n",
        "        if (continue_training and last_itr < itr ) or continue_training==False: #recover the training from the last step or train a new model\n",
        "          loss_dict = model(images, targets)\n",
        "          print(\"loss dict\")\n",
        "          print(loss_dict)\n",
        "          print(\" \")\n",
        "          losses = sum(loss for loss in loss_dict.values())\n",
        "          if custom:\n",
        "            losses = loss_dict['loss_classifier']*2.0 + loss_dict['loss_box_reg'] * 2.5 + loss_dict['loss_rpn_box_reg']\n",
        "          loss_value = losses.item()\n",
        "\n",
        "          print(\"loss value\")\n",
        "          print(loss_value)\n",
        "          print(\" \")\n",
        "\n",
        "          print(\"iteration\", itr)\n",
        "\n",
        "          loss_hist.send(loss_value)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          losses.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if itr % 50 == 0:\n",
        "              print(f\"Iteration #{itr} loss: {loss_value}\")\n",
        "          if itr % 250 == 0:\n",
        "            print(\"sto salvando il modello\")\n",
        "            if custom:\n",
        "              torch.save(model.state_dict(), \"/content/drive/MyDrive/AML_Project_2021/Modelli/custom\"+str(itr)+\".pth\")\n",
        "            else:\n",
        "              torch.save(model.state_dict(), \"/content/drive/MyDrive/AML_Project_2021/Modelli/base\"+str(itr)+\".pth\")\n",
        "        itr += 1\n",
        "        print(\"iterazione itr\", itr)\n",
        "    # update the learning rate\n",
        "    if lr_scheduler is not None:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#unlycky case where there is no box but just background mask\n",
        "images_b =[]; targets_b = []\n",
        "for t in range(len(targets)):\n",
        "  if targets[t]['boxes'].shape == torch.Size([0]):\n",
        "    continue  #no box just background\n",
        "  images_b.append(images[t])\n",
        "  targets_b.append(targets[t])\n",
        "\n",
        "if len(targets_b) == 0:\n",
        "  continue \n",
        "\n",
        "\"\"\"    "
      ],
      "metadata": {
        "id": "-FeyEUcoLGBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "URiNY8KRVVDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!tar -tvf /content/drive/MyDrive/AML_Project_2021/nuimages-v1.0-all-samples.tgz"
      ],
      "metadata": {
        "id": "cpmoxleR8CGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one time only\n",
        "#!tar -xvf /content/drive/MyDrive/AML_Project_2021/nuimages-v1.0-all-samples.tgz samples/CAM_FRONT_LEFT/ -C /content/drive/MyDrive/AML_Project_2021/Data/"
      ],
      "metadata": {
        "id": "0hF7PgCjx_WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!mv samples/CAM_FRONT_LEFT /content/drive/MyDrive/AML_Project_2021/Metadata/samples/"
      ],
      "metadata": {
        "id": "LPE7BQ8yn32U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!find /content/drive/MyDrive/AML_Project_2021/Metadata/samples/ -type f | wc -l"
      ],
      "metadata": {
        "id": "Bd2pYJvQAUB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZC1tWDJ3uvSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# codice per la raccolta del loss sui modelli"
      ],
      "metadata": {
        "id": "D9NdyOBIbZto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the idea is to load the different stored models and calculate their loss on 100 images and their accuracy and loss on 100 validation images"
      ],
      "metadata": {
        "id": "5CQOYJEGbgrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "77j3UNp5k0G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss on 100 immages\n",
        "#first of all get the model \n",
        "current_model = 250\n",
        "last_model_itr = 10000\n",
        "dizionario_loss = {}\n",
        "with torch.no_grad():\n",
        "\n",
        "  while (current_model < last_model_itr):\n",
        "\n",
        "    #break #TO DO REMOVE WHEN NEED ----------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    if custom:\n",
        "      model.load_state_dict( torch.load( \"/content/drive/MyDrive/AML_Project_2021/Modelli/custom\"+str(current_model)+\".pth\")) #if on CPU, add this: CPU map_location=torch.device('cpu')\n",
        "    else:\n",
        "      model.load_state_dict( torch.load( \"/content/drive/MyDrive/AML_Project_2021/Modelli/base\"+str(current_model)+\".pth\")) #if on CPU, add this: CPU map_location=torch.device('cpu')\n",
        "    loss = 0\n",
        "    i = 0\n",
        "    \n",
        "    for images, targets in train_data_loader:  \n",
        "\n",
        "      #same steps as in training for cleaning the images\n",
        "      images = list(image.to(device) for image in images)\n",
        "      targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "      skip=False\n",
        "      for t in range(len(targets)):\n",
        "        if targets[t]['boxes'].shape == torch.Size([0]):\n",
        "          skip=True\n",
        "          break  #no box just background\n",
        "      if skip or len(targets) == 0:\n",
        "          continue\n",
        "      loss_dict = model(images, targets)\n",
        "      print(\"loss dict\")\n",
        "      print(loss_dict)\n",
        "      print(\" \")\n",
        "      losses = sum(loss for loss in loss_dict.values())\n",
        "      if custom:\n",
        "        losses = loss_dict['loss_classifier']*2.0 + loss_dict['loss_box_reg'] * 2.5 + loss_dict['loss_rpn_box_reg']\n",
        "      loss_value = losses.item()\n",
        "\n",
        "      print(\"loss value\")\n",
        "      print(loss_value)\n",
        "      print(\" \")\n",
        "      print(\" \")\n",
        "      print(\"iterazione \",i)\n",
        "      print(\" \")\n",
        "      loss += loss_value\n",
        "      i+=1\n",
        "      if (i > 100):\n",
        "        break\n",
        "    #loop esterno\n",
        "    loss_fin = loss/100\n",
        "    dizionario_loss[current_model]=loss_fin\n",
        "    with open('/content/drive/MyDrive/AML_Project_2021/Modelli/loss'+str(custom)+'.json', 'w') as fp:\n",
        "      json.dump(dizionario_loss, fp)\n",
        "    current_model += 250\n",
        "    loss = 0\n",
        "    print(\"ho salvato loss di \", loss_fin, \" del modello numero \", current_model-250)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "7osaKoQib9kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluation\n"
      ],
      "metadata": {
        "id": "yxXcyaRYu5te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload the model. Please instantiate the model first"
      ],
      "metadata": {
        "id": "aafX03Ld70Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if custom:\n",
        "  model.load_state_dict( torch.load( \"/content/drive/MyDrive/AML_Project_2021/Modelli/custom\"+str(3500)+\".pth\")) #if on CPU, add this: CPU map_location=torch.device('cpu')\n",
        "else:\n",
        "  model.load_state_dict( torch.load( \"/content/drive/MyDrive/AML_Project_2021/Modelli/base\"+str(3500)+\".pth\")) #if on CPU, add this: CPU map_location=torch.device('cpu')"
      ],
      "metadata": {
        "id": "7poa225ibRQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates the dataframe on evaluation. Initialize valid_data_loader first"
      ],
      "metadata": {
        "id": "CPsge5qk8KGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "FRTIx5pIz9X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag = True # Set to true to stop early\n",
        "max_cnt = 10           # Stops after max_cnt iteration if flag = True\n",
        "cnt = 0;\n",
        "\n",
        "#Create df\n",
        "new_df = pd.DataFrame(columns=[\"boxes\", \"labels\", \"scores\", \"target_boxes\", \"target_labels\"])\n",
        "\n",
        "#Iterate\n",
        "model.eval()\n",
        "for images, targets in train_data_loader:\n",
        "\n",
        "\n",
        "  images = list(img.to(device) for img in images)\n",
        "  targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "  boxes = targets[0]['boxes'].cpu().numpy().astype(np.int32)\n",
        "  sample = images[0].permute(1,2,0).cpu().numpy()\n",
        "  \n",
        "  cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "  \n",
        "  \n",
        "  outputs = model(images)\n",
        "  print(\"iteration :\",cnt)\n",
        "  outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "  \n",
        "  new_df = new_df.append({\"boxes\" : outputs[0][\"boxes\"].detach().numpy(), \"labels\" : outputs[0][\"labels\"], \"scores\": outputs[0][\"scores\"].detach().numpy(), #there was [outputs[0][\"boxes\"]]\n",
        "                          \"target_boxes\" : torch.tensor(boxes), \"target_labels\": targets[0][\"labels\"],\"img_idx\" : cnt},  #okkio\n",
        "                         ignore_index=True)\n",
        "\n",
        "  cnt+=1\n",
        "  if flag:\n",
        "    if cnt==max_cnt:\n",
        "      break\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bn6WVkILwHDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d525682e-6a78-4c61-e87a-c97c6fd3ab7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing object annotations:\n",
            "\n",
            "Printing surface annotations:\n",
            "a6351bb648b254f6a2e5afbb2fc07de6 flat.driveable_surface\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of x is  torch.Size([1000, 256, 7, 7])\n",
            "the shape of x is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "the shape of mat1 is ---------------------------------------------------------------------------------: torch.Size([1000, 49, 49])\n",
            "the shape of mat2 is ---------------------------------------------------------------------------------: torch.Size([1000, 512, 49])\n",
            "the shape of out is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "iteration : 0\n",
            "Printing object annotations:\n",
            "85463f57d63748c1aa2075800eb41544 vehicle.bicycle ['cycle.without_rider']\n",
            "d8de6f716a754d3aae803974f08adea5 movable_object.barrier []\n",
            "\n",
            "Printing surface annotations:\n",
            "2d83247e19c35e5bbfcbd54a40e0c244 flat.driveable_surface\n",
            "shape of x is  torch.Size([1000, 256, 7, 7])\n",
            "the shape of x is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "the shape of mat1 is ---------------------------------------------------------------------------------: torch.Size([1000, 49, 49])\n",
            "the shape of mat2 is ---------------------------------------------------------------------------------: torch.Size([1000, 512, 49])\n",
            "the shape of out is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "iteration : 1\n",
            "Printing object annotations:\n",
            "1c0e6e5b52dd47bc9d03843478dedbad static_object.bicycle_rack []\n",
            "1e38b0ec0f6a445e9364588c27325721 movable_object.trafficcone []\n",
            "\n",
            "Printing surface annotations:\n",
            "b09d09ea399a56b6b4e2b49ea079d212 flat.driveable_surface\n",
            "shape of x is  torch.Size([1000, 256, 7, 7])\n",
            "the shape of x is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "the shape of mat1 is ---------------------------------------------------------------------------------: torch.Size([1000, 49, 49])\n",
            "the shape of mat2 is ---------------------------------------------------------------------------------: torch.Size([1000, 512, 49])\n",
            "the shape of out is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "iteration : 2\n",
            "Printing object annotations:\n",
            "\n",
            "Printing surface annotations:\n",
            "shape of x is  torch.Size([1000, 256, 7, 7])\n",
            "the shape of x is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "the shape of mat1 is ---------------------------------------------------------------------------------: torch.Size([1000, 49, 49])\n",
            "the shape of mat2 is ---------------------------------------------------------------------------------: torch.Size([1000, 512, 49])\n",
            "the shape of out is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "iteration : 3\n",
            "Printing object annotations:\n",
            "15d7f53c7c294eecb3bbcb44925e4dc7 human.pedestrian.adult ['pedestrian.standing']\n",
            "2d035d09b50b4944818d39fecc465e38 human.pedestrian.adult ['pedestrian.standing']\n",
            "3ba79401417e49459d7733621eba65e6 vehicle.motorcycle ['cycle.without_rider']\n",
            "490fccffd7ce46e78cef12084d781ce8 movable_object.trafficcone []\n",
            "80d34fbe094c426b941e0e149970275b human.pedestrian.adult ['pedestrian.standing']\n",
            "8e2f1a5fb55946c189ed64a654e15971 human.pedestrian.adult ['pedestrian.standing']\n",
            "90ea52f526204a7db50928316bc4d7d3 human.pedestrian.adult ['pedestrian.standing']\n",
            "9505a1a7a3fc44b5b30955b484140fa4 human.pedestrian.adult ['pedestrian.standing']\n",
            "9bf324c4639c4a398c797809aa90aeaa human.pedestrian.adult ['pedestrian.moving']\n",
            "aa85afa7ca2749be9461f913c74df5e9 human.pedestrian.adult ['pedestrian.standing']\n",
            "b150605c155f4540ba9355d7c375cfd5 human.pedestrian.adult ['pedestrian.sitting_lying_down']\n",
            "bbef1ca5abd04621ba92ed7eb6766236 human.pedestrian.adult ['pedestrian.moving']\n",
            "d55e9e9e4c954fd5bf384fa6ec1622bb human.pedestrian.adult ['pedestrian.moving']\n",
            "f59b93c9e98d433d8dfcdb500789fee6 human.pedestrian.adult ['pedestrian.standing']\n",
            "\n",
            "Printing surface annotations:\n",
            "315edb913b7e5e3686de4f401d2c1406 flat.driveable_surface\n",
            "shape of x is  torch.Size([1000, 256, 7, 7])\n",
            "the shape of x is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "the shape of mat1 is ---------------------------------------------------------------------------------: torch.Size([1000, 49, 49])\n",
            "the shape of mat2 is ---------------------------------------------------------------------------------: torch.Size([1000, 512, 49])\n",
            "the shape of out is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "iteration : 4\n",
            "Printing object annotations:\n",
            "2b4da119f51c4076bc29d19178761ffe vehicle.car ['vehicle.parked']\n",
            "4625a70a683f46c288c016ec549ab47a vehicle.car ['vehicle.parked']\n",
            "8cd92de0c91b40a299bb53fad1ee6055 vehicle.car ['vehicle.moving']\n",
            "93d6bf1785ef4507abaf21a5173c4b28 static_object.bicycle_rack []\n",
            "e700957ac3764fef9b0ed9d616e0cbe3 vehicle.truck ['vehicle.moving']\n",
            "\n",
            "Printing surface annotations:\n",
            "shape of x is  torch.Size([1000, 256, 7, 7])\n",
            "the shape of x is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "the shape of mat1 is ---------------------------------------------------------------------------------: torch.Size([1000, 49, 49])\n",
            "the shape of mat2 is ---------------------------------------------------------------------------------: torch.Size([1000, 512, 49])\n",
            "the shape of out is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "iteration : 5\n",
            "Printing object annotations:\n",
            "55d652f33050450cb80439b988f648f8 vehicle.car ['vehicle.parked']\n",
            "9468d1e2a1ad459fb1054634e92aa436 vehicle.car ['vehicle.parked']\n",
            "b055b5086b484debb681d91a4e1cd79f vehicle.car ['vehicle.parked']\n",
            "\n",
            "Printing surface annotations:\n",
            "188b01906ba05af496d9614598f6b61e flat.driveable_surface\n",
            "shape of x is  torch.Size([1000, 256, 7, 7])\n",
            "the shape of x is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "the shape of mat1 is ---------------------------------------------------------------------------------: torch.Size([1000, 49, 49])\n",
            "the shape of mat2 is ---------------------------------------------------------------------------------: torch.Size([1000, 512, 49])\n",
            "the shape of out is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "iteration : 6\n",
            "Printing object annotations:\n",
            "06aece782e4d4cf192203f5c280f0c30 vehicle.car ['vehicle.parked']\n",
            "2f8ff72849794e4eb426b1f07a0f8a33 vehicle.bus.rigid ['vehicle.parked']\n",
            "ce19359b6d564a94bb045ddbefb336fe vehicle.bus.rigid ['vehicle.parked']\n",
            "\n",
            "Printing surface annotations:\n",
            "e2a57cbe30905861bb7a154c7702d1f0 flat.driveable_surface\n",
            "shape of x is  torch.Size([1000, 256, 7, 7])\n",
            "the shape of x is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "the shape of mat1 is ---------------------------------------------------------------------------------: torch.Size([1000, 49, 49])\n",
            "the shape of mat2 is ---------------------------------------------------------------------------------: torch.Size([1000, 512, 49])\n",
            "the shape of out is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "iteration : 7\n",
            "Printing object annotations:\n",
            "04d408a2ab9f46289f7a250adb5e5218 human.pedestrian.adult ['pedestrian.sitting_lying_down']\n",
            "1eed4574b3ff45bbbbecffce4f9c8c56 human.pedestrian.adult ['pedestrian.moving']\n",
            "c21e4f894a81465b84b5a2a040174f6b human.pedestrian.adult ['pedestrian.standing']\n",
            "f3afd9a7edd344539a4245d1561a3fab human.pedestrian.adult ['pedestrian.moving']\n",
            "\n",
            "Printing surface annotations:\n",
            "d4b355d98f1552cb8cdd1af0ad93828c flat.driveable_surface\n",
            "shape of x is  torch.Size([1000, 256, 7, 7])\n",
            "the shape of x is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "the shape of mat1 is ---------------------------------------------------------------------------------: torch.Size([1000, 49, 49])\n",
            "the shape of mat2 is ---------------------------------------------------------------------------------: torch.Size([1000, 512, 49])\n",
            "the shape of out is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "iteration : 8\n",
            "Printing object annotations:\n",
            "0df28708c53b4d6b853b9491544fd865 human.pedestrian.adult ['pedestrian.moving']\n",
            "1148e257ac6c42e896e43afeea7caf8c vehicle.car ['vehicle.moving']\n",
            "564ce10edbcf486581af14c2a899e739 vehicle.truck ['vehicle.moving']\n",
            "7e77d435daa44991bf49df6f8417a1ec human.pedestrian.adult ['pedestrian.moving']\n",
            "\n",
            "Printing surface annotations:\n",
            "7468af97078c596895457563f9a84449 flat.driveable_surface\n",
            "b6942a886248569bb988883d463eea32 vehicle.ego\n",
            "shape of x is  torch.Size([1000, 256, 7, 7])\n",
            "the shape of x is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "the shape of mat1 is ---------------------------------------------------------------------------------: torch.Size([1000, 49, 49])\n",
            "the shape of mat2 is ---------------------------------------------------------------------------------: torch.Size([1000, 512, 49])\n",
            "the shape of out is ---------------------------------------------------------------------------------: torch.Size([1000, 1024, 7, 7])\n",
            "iteration : 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save df; WIP since it transforms tensors and arrays to Strings"
      ],
      "metadata": {
        "id": "_dl-B0Y38vge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#new_df.to_csv(\"/content/drive/MyDrive/def_AML.csv\",index= False)"
      ],
      "metadata": {
        "id": "0GebVCMYjuV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload the df (don't)"
      ],
      "metadata": {
        "id": "-lenXX4x80Dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#new_df = pd.read_csv(\"/content/drive/MyDrive/prova_AML.csv\") #but then tensor become string..."
      ],
      "metadata": {
        "id": "uMrxJ_-KuVSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrics for boxes"
      ],
      "metadata": {
        "id": "Mjsi2aKes3nI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "-m9ZZ5Iss1JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility functions"
      ],
      "metadata": {
        "id": "6XRZCNMA87JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_scores(pred_boxes):\n",
        "    \"\"\"Creates a dictionary of from model_scores to image ids.\n",
        "    Args:\n",
        "        pred_boxes (dict): dict of dicts of 'boxes' and 'scores'\n",
        "        dizionario fatto: {scores: [], boxes: []}\n",
        "        dizionario fatto: img_id : {scores: [], boxes: []}\n",
        "    Returns:\n",
        "        dict: keys are model_scores and values are image ids (usually filenames)\n",
        "    \"\"\"\n",
        "    model_score={}\n",
        "    for img_id, val in pred_boxes.items():\n",
        "        #print(\"---------\",val)\n",
        "        for score in val['scores']:\n",
        "            if score not in model_score.keys():\n",
        "                model_score[score]=[img_id]\n",
        "            else:\n",
        "                model_score[score].append(img_id)\n",
        "    return model_score"
      ],
      "metadata": {
        "id": "h5yu44mds7BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input per get_model_Scores\n",
        "def obtain_dict_of_dict(dd):\n",
        "  new_dict = {}\n",
        "  for x in range(len(dd[\"boxes\"])):\n",
        "    value_b = []\n",
        "    value_s = []\n",
        "    for y in dd[\"boxes\"].keys():\n",
        "      if y == x:\n",
        "        value_b = dd[\"boxes\"][y]\n",
        "    for z in dd[\"scores\"].keys():\n",
        "      if z == x:\n",
        "        value_s = dd[\"scores\"][z]\n",
        "    new_dict[x] = {\"boxes\" : value_b, \"scores\" : value_s}\n",
        "  return(new_dict)"
      ],
      "metadata": {
        "id": "YoJc3sNz3AwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input dopo forse\n",
        "def obtain_dict_from_img(new_df, id):\n",
        "  new_dict = {\"boxes\" :  new_df.loc[id, \"boxes\"], \"scores\" :  new_df.loc[id, \"scores\"]}\n",
        "  return(new_dict)"
      ],
      "metadata": {
        "id": "PGj5eBlw_0Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#! IMPORTANT\n",
        "def dict_numbox_box(df, idx):\n",
        "  newd={}\n",
        "  actual = df.loc[idx, \"boxes\"]\n",
        "  #print(actual)\n",
        "  #for x in actual:\n",
        "  for i,y in enumerate(actual):\n",
        "        #print(i, torch.tensor(y))\n",
        "        newd[torch.tensor(y)]=i\n",
        "  return(newd)"
      ],
      "metadata": {
        "id": "5Uerx-DGwRx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_iou( gt_bbox, pred_bbox):\n",
        "    '''\n",
        "    This function takes the predicted bounding box and ground truth bounding box and \n",
        "    return the IoU ratio\n",
        "    '''\n",
        "    #print(pred_bbox)\n",
        "    x_topleft_gt, y_topleft_gt, x_bottomright_gt, y_bottomright_gt= gt_bbox\n",
        "    x_topleft_p, y_topleft_p, x_bottomright_p, y_bottomright_p= pred_bbox\n",
        "  \n",
        "    \n",
        "    if (x_topleft_gt > x_bottomright_gt) or (y_topleft_gt> y_bottomright_gt):\n",
        "        raise AssertionError(\"Ground Truth Bounding Box is not correct\")\n",
        "    if (x_topleft_p > x_bottomright_p) or (y_topleft_p> y_bottomright_p):\n",
        "        raise AssertionError(\"Predicted Bounding Box is not correct\",x_topleft_p, x_bottomright_p,y_topleft_p,y_bottomright_gt)\n",
        "        \n",
        "         \n",
        "    #if the GT bbox and predcited BBox do not overlap then iou=0\n",
        "    if(x_bottomright_gt< x_topleft_p):\n",
        "        # If bottom right of x-coordinate  GT  bbox is less than or above the top left of x coordinate of  the predicted BBox\n",
        "        \n",
        "        return 0.0\n",
        "    if(y_bottomright_gt< y_topleft_p):  # If bottom right of y-coordinate  GT  bbox is less than or above the top left of y coordinate of  the predicted BBox\n",
        "        \n",
        "        return 0.0\n",
        "    if(x_topleft_gt> x_bottomright_p): # If bottom right of x-coordinate  GT  bbox is greater than or below the bottom right  of x coordinate of  the predcited BBox\n",
        "        \n",
        "        return 0.0\n",
        "    if(y_topleft_gt> y_bottomright_p): # If bottom right of y-coordinate  GT  bbox is greater than or below the bottom right  of y coordinate of  the predcited BBox\n",
        "        \n",
        "        return 0.0\n",
        "    \n",
        "    \n",
        "    GT_bbox_area = (x_bottomright_gt -  x_topleft_gt + 1) * (  y_bottomright_gt -y_topleft_gt + 1)\n",
        "    Pred_bbox_area =(x_bottomright_p - x_topleft_p + 1 ) * ( y_bottomright_p -y_topleft_p + 1)\n",
        "    \n",
        "    x_top_left =np.max([x_topleft_gt, x_topleft_p])\n",
        "    y_top_left = np.max([y_topleft_gt, y_topleft_p])\n",
        "    x_bottom_right = np.min([x_bottomright_gt, x_bottomright_p])\n",
        "    y_bottom_right = np.min([y_bottomright_gt, y_bottomright_p])\n",
        "    \n",
        "    intersection_area = (x_bottom_right- x_top_left + 1) * (y_bottom_right-y_top_left  + 1)\n",
        "    \n",
        "    union_area = (GT_bbox_area + Pred_bbox_area - intersection_area)\n",
        "   \n",
        "    return intersection_area/union_area"
      ],
      "metadata": {
        "id": "HxR1e6sBtbdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_single_image_results(gt_boxes, pred_boxes, iou_thr):\n",
        "    \"\"\"Calculates number of true_pos, false_pos, false_neg from single batch of boxes.\n",
        "    Args:\n",
        "        gt_boxes (list of list of floats): list of locations of ground truth\n",
        "            objects as [xmin, ymin, xmax, ymax]\n",
        "        pred_boxes (dict): dict of dicts of 'boxes' (formatted like `gt_boxes`)\n",
        "            and 'scores'\n",
        "        iou_thr (float): value of IoU to consider as threshold for a\n",
        "            true prediction.\n",
        "    Returns:\n",
        "        dict: true positives (int), false positives (int), false negatives (int)\n",
        "    \"\"\"\n",
        "    all_pred_indices= range(len(pred_boxes))\n",
        "    all_gt_indices=range(len(gt_boxes))\n",
        "    if len(all_pred_indices)==0:\n",
        "        tp=0\n",
        "        fp=0\n",
        "        fn=0\n",
        "        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
        "    if len(all_gt_indices)==0:\n",
        "        tp=0\n",
        "        fp=0\n",
        "        fn=0\n",
        "        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
        "    \n",
        "    gt_idx_thr=[]\n",
        "    pred_idx_thr=[]\n",
        "    ious=[]\n",
        "    \n",
        "    #it's enumerating the keys of dict (?)\n",
        "    for ipb, pred_box in enumerate(pred_boxes):\n",
        "        #print(\"-------------\",ipb, pred_box)\n",
        "        \n",
        "        for igp, gt_box in enumerate(gt_boxes):#for igp, gt_box in enumerate(gt_boxes):\n",
        "            #print(igb, pred_box)\n",
        "            #print(\"------------------------\",pred_box)\n",
        "            iou= calc_iou(list(gt_box), pred_box)\n",
        "            \n",
        "            if iou >iou_thr:\n",
        "                gt_idx_thr.append( igp)#igb)\n",
        "                pred_idx_thr.append(ipb) #ipb)\n",
        "                ious.append(iou)\n",
        "          \n",
        "    iou_sort = np.argsort(ious)[::1]\n",
        "    if len(iou_sort)==0:\n",
        "        tp=0\n",
        "        fp=0\n",
        "        fn=0\n",
        "        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
        "    else:\n",
        "        gt_match_idx=[]\n",
        "        pred_match_idx=[]\n",
        "        for idx in iou_sort:\n",
        "            gt_idx=gt_idx_thr[idx]\n",
        "            pr_idx= pred_idx_thr[idx]\n",
        "            # If the boxes are unmatched, add them to matches\n",
        "            if(gt_idx not in gt_match_idx) and (pr_idx not in pred_match_idx):\n",
        "                gt_match_idx.append(gt_idx)\n",
        "                pred_match_idx.append(pr_idx)\n",
        "        tp= len(gt_match_idx)\n",
        "        fp= len(pred_boxes) - len(pred_match_idx)\n",
        "        fn = len(gt_boxes) - len(gt_match_idx)\n",
        "    return {'true_positive': tp, 'false_positive': fp, 'false_negative': fn}"
      ],
      "metadata": {
        "id": "kv-vMqSwu7It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just to test the get_single_image_result"
      ],
      "metadata": {
        "id": "SjXaogBOzJSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(len(new_df)):\n",
        "      gt_boxes = new_df.loc[x, \"target_boxes\"]#nd array, kinda ok\n",
        "      pred_boxes = dict_numbox_box(new_df, x)\n",
        "      print(\"img result\", get_single_image_results(gt_boxes, pred_boxes, 0.5))\n",
        "      "
      ],
      "metadata": {
        "id": "3W_q_bfc4-cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fd2ea56-17df-499b-ec67-a6d20791e703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img result {'true_positive': 0, 'false_positive': 0, 'false_negative': 0}\n",
            "img result {'true_positive': 2, 'false_positive': 64, 'false_negative': 0}\n",
            "img result {'true_positive': 2, 'false_positive': 30, 'false_negative': 0}\n",
            "img result {'true_positive': 0, 'false_positive': 0, 'false_negative': 0}\n",
            "img result {'true_positive': 11, 'false_positive': 68, 'false_negative': 3}\n",
            "img result {'true_positive': 4, 'false_positive': 33, 'false_negative': 1}\n",
            "img result {'true_positive': 3, 'false_positive': 25, 'false_negative': 0}\n",
            "img result {'true_positive': 3, 'false_positive': 17, 'false_negative': 0}\n",
            "img result {'true_positive': 2, 'false_positive': 36, 'false_negative': 2}\n",
            "img result {'true_positive': 4, 'false_positive': 24, 'false_negative': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_precision_recall(image_results):\n",
        "    \"\"\"Calculates precision and recall from the set of images\n",
        "    Args:\n",
        "        img_results (dict): dictionary formatted like:\n",
        "            {\n",
        "                'img_id1': {'true_pos': int, 'false_pos': int, 'false_neg': int},\n",
        "                'img_id2': ...\n",
        "                ...\n",
        "            }\n",
        "    Returns:\n",
        "        tuple: of floats of (precision, recall)\n",
        "    \"\"\"\n",
        "    true_positive=0\n",
        "    false_positive=0\n",
        "    false_negative=0\n",
        "    for img_id, res in image_results.items():\n",
        "        true_positive +=res['true_positive']\n",
        "        false_positive += res['false_positive']\n",
        "        false_negative += res['false_negative']\n",
        "        try:\n",
        "            precision = true_positive/(true_positive+ false_positive)\n",
        "        except ZeroDivisionError:\n",
        "            precision=0.0\n",
        "        try:\n",
        "            recall = true_positive/(true_positive + false_negative)\n",
        "        except ZeroDivisionError:\n",
        "            recall=0.0\n",
        "    return (precision, recall)"
      ],
      "metadata": {
        "id": "J1SshE8Itfg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a dict with all the images TP, FP etc."
      ],
      "metadata": {
        "id": "_HUK2JTkzAw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_dict = {}\n",
        "for x in range(len(new_df)):\n",
        "      gt_boxes = new_df.loc[x, \"target_boxes\"]#nd array, kinda ok\n",
        "      pred_boxes = dict_numbox_box(new_df, x)\n",
        "      \n",
        "      img_dict[x] = get_single_image_results(gt_boxes, pred_boxes, 0.5) #<---- modify threshold here as desired"
      ],
      "metadata": {
        "id": "ZCJHaiRtzAKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision and recall over all the images"
      ],
      "metadata": {
        "id": "r5NPKyrD7IMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(calc_precision_recall(img_dict))"
      ],
      "metadata": {
        "id": "WAgTHOzCzcZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4abd1730-fe1f-44a9-8204-4fa7a2da91d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.09451219512195122, 0.8378378378378378)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def  get_avg_precision_at_iou(gt_boxes, pred_bb, iou_thr=0.5):\n",
        "    \n",
        "    model_scores = get_model_scores(pred_bb)\n",
        "    sorted_model_scores= sorted(model_scores.keys())# Sort the predicted boxes in descending order (lowest scoring boxes first):\n",
        "    for img_id in pred_bb.keys():\n",
        "        #print(img_id)\n",
        "        \n",
        "        arg_sort = np.argsort(pred_bb[img_id]['scores'])\n",
        "        #print( np.array(pred_bb[img_id]['boxes'])[arg_sort])\n",
        "        pred_bb[img_id]['scores'] = np.array(pred_bb[img_id]['scores'])[arg_sort].tolist()\n",
        "        pred_bb[img_id]['boxes'] = np.array(pred_bb[img_id]['boxes'])[arg_sort].tolist()\n",
        "        pred_boxes_pruned = deepcopy(pred_bb)\n",
        "    \n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    model_thrs = []\n",
        "    img_results = {}# Loop over model score thresholds and calculate precision, recall\n",
        "    for ithr, model_score_thr in enumerate(sorted_model_scores[:-1]):\n",
        "            # On first iteration, define img_results for the first time:\n",
        "        print(\"Model score : \", model_score_thr)\n",
        "        img_ids = gt_boxes.keys() if ithr == 0 else model_scores[model_score_thr] \n",
        "        for img_id in img_ids:\n",
        "               \n",
        "            gt_boxes_img = gt_boxes[img_id]\n",
        "            box_scores = pred_boxes_pruned[img_id]['scores']\n",
        "            start_idx = 0\n",
        "            for score in box_scores:\n",
        "                if score <= model_score_thr:\n",
        "                    pred_boxes_pruned[img_id]\n",
        "                    start_idx += 1\n",
        "                else:\n",
        "                    break \n",
        "            # Remove boxes, scores of lower than threshold scores:\n",
        "            pred_boxes_pruned[img_id]['scores']= pred_boxes_pruned[img_id]['scores'][start_idx:]\n",
        "            pred_boxes_pruned[img_id]['boxes']= pred_boxes_pruned[img_id]['boxes'][start_idx:]# Recalculate image results for this image\n",
        "            print(img_id)\n",
        "            img_results[img_id] = get_single_image_results(gt_boxes_img, pred_boxes_pruned[img_id]['boxes'], iou_thr=0.5)# calculate precision and recall\n",
        "        prec, rec = calc_precision_recall(img_results)\n",
        "        precisions = np.append(precisions, prec) #precisions.append(prec)\n",
        "        recalls.append(rec)\n",
        "        model_thrs.append(model_score_thr)\n",
        "        precisions = np.array(precisions)\n",
        "    recalls = np.array(recalls)\n",
        "    prec_at_rec = []\n",
        "    for recall_level in np.linspace(0.0, 1.0, 11):\n",
        "        try:\n",
        "            args= np.argwhere(recalls>recall_level).flatten()\n",
        "            prec= max(precisions[args])\n",
        "            print(recalls,\"Recall\")\n",
        "            print(      recall_level,\"Recall Level\")\n",
        "            print(       args, \"Args\")\n",
        "            print(       prec, \"precision\")\n",
        "        except ValueError:\n",
        "            prec=0.0\n",
        "        prec_at_rec.append(prec)\n",
        "    avg_prec = np.mean(prec_at_rec) \n",
        "    return {\n",
        "        'avg_prec': avg_prec,\n",
        "        'precisions': precisions,\n",
        "        'recalls': recalls,\n",
        "        'model_thrs': model_thrs}"
      ],
      "metadata": {
        "id": "eGiVskH0vAiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = get_avg_precision_at_iou(new_df[\"target_boxes\"],obtain_dict_of_dict(new_df[[\"boxes\", \"scores\", \"img_idx\"]]), 0.5) #<----- Modify threshold here as desired\n"
      ],
      "metadata": {
        "id": "kPtaVcau0n_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "rU4mymFcx1W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bddDmFGiPzih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#TODO TEST\n",
        "model.eval()\n",
        "max_cnt = 100\n",
        "model_itr = 250\n",
        "max_model_itr = 8250\n",
        "avg_scores = {}\n",
        "with torch.no_grad():\n",
        "\n",
        "  while(model_itr < max_model_itr):\n",
        "    \n",
        "    if custom:\n",
        "      model.load_state_dict( torch.load( \"/content/drive/MyDrive/AML_Project_2021/Modelli/custom\"+str(model_itr)+\".pth\")) #if on CPU, add this: CPU map_location=torch.device('cpu')\n",
        "    else:\n",
        "      model.load_state_dict( torch.load( \"/content/drive/MyDrive/AML_Project_2021/Modelli/base\"+str(model_itr)+\".pth\")) #if on CPU, add this: CPU map_location=torch.device('cpu')\n",
        "    \n",
        "\n",
        "    #Create df\n",
        "    new_df = pd.DataFrame(columns=[\"boxes\", \"labels\", \"scores\", \"target_boxes\", \"target_labels\"])\n",
        "\n",
        "    #Iterate\n",
        "    cnt = 0\n",
        "    for images, targets in train_data_loader:\n",
        "\n",
        "\n",
        "      images = list(img.to(device) for img in images)\n",
        "      targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "      boxes = targets[0]['boxes'].cpu().numpy().astype(np.int32)\n",
        "        \n",
        "      \n",
        "      outputs = model(images)\n",
        "      print(\"iteration :\",cnt)\n",
        "      outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "      \n",
        "      new_df = new_df.append({\"boxes\" : outputs[0][\"boxes\"].detach().numpy(), \"labels\" : outputs[0][\"labels\"], \"scores\": outputs[0][\"scores\"].detach().numpy(), #there was [outputs[0][\"boxes\"]]\n",
        "                              \"target_boxes\" : torch.tensor(boxes), \"target_labels\": targets[0][\"labels\"],\"img_idx\" : cnt},  #okkio\n",
        "                            ignore_index=True)\n",
        "\n",
        "      cnt+=1\n",
        "      if flag:\n",
        "        if cnt==max_cnt:\n",
        "          break\n",
        "    results = get_avg_precision_at_iou(new_df[\"target_boxes\"],obtain_dict_of_dict(new_df[[\"boxes\", \"scores\", \"img_idx\"]]), 0.5)\n",
        "    avg_scores[model_itr] = results['avg_prec']\n",
        "    with open('/content/drive/MyDrive/AML_Project_2021/Modelli/acc_val'+str(custom)+'.json', 'w') as fp:\n",
        "      json.dump(avg_scores, fp)\n",
        "    model_itr += 250\n",
        "\"\"\"\n",
        "    "
      ],
      "metadata": {
        "id": "fWoBTFB6OF8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2fc9388-6c7c-4509-a936-6d714aebd82f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#TODO TEST\\nmodel.eval()\\nmax_cnt = 100\\nmodel_itr = 250\\nmax_model_itr = 8250\\navg_scores = {}\\nwith torch.no_grad():\\n\\n  while(model_itr < max_model_itr):\\n    \\n    if custom:\\n      model.load_state_dict( torch.load( \"/content/drive/MyDrive/AML_Project_2021/Modelli/custom\"+str(model_itr)+\".pth\")) #if on CPU, add this: CPU map_location=torch.device(\\'cpu\\')\\n    else:\\n      model.load_state_dict( torch.load( \"/content/drive/MyDrive/AML_Project_2021/Modelli/base\"+str(model_itr)+\".pth\")) #if on CPU, add this: CPU map_location=torch.device(\\'cpu\\')\\n    \\n\\n    #Create df\\n    new_df = pd.DataFrame(columns=[\"boxes\", \"labels\", \"scores\", \"target_boxes\", \"target_labels\"])\\n\\n    #Iterate\\n    cnt = 0\\n    for images, targets in train_data_loader:\\n\\n\\n      images = list(img.to(device) for img in images)\\n      targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\\n\\n      boxes = targets[0][\\'boxes\\'].cpu().numpy().astype(np.int32)\\n        \\n      \\n      outputs = model(images)\\n      print(\"iteration :\",cnt)\\n      outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\\n      \\n      new_df = new_df.append({\"boxes\" : outputs[0][\"boxes\"].detach().numpy(), \"labels\" : outputs[0][\"labels\"], \"scores\": outputs[0][\"scores\"].detach().numpy(), #there was [outputs[0][\"boxes\"]]\\n                              \"target_boxes\" : torch.tensor(boxes), \"target_labels\": targets[0][\"labels\"],\"img_idx\" : cnt},  #okkio\\n                            ignore_index=True)\\n\\n      cnt+=1\\n      if flag:\\n        if cnt==max_cnt:\\n          break\\n    results = get_avg_precision_at_iou(new_df[\"target_boxes\"],obtain_dict_of_dict(new_df[[\"boxes\", \"scores\", \"img_idx\"]]), 0.5)\\n    avg_scores[model_itr] = results[\\'avg_prec\\']\\n    with open(\\'/content/drive/MyDrive/AML_Project_2021/Modelli/acc_val\\'+str(custom)+\\'.json\\', \\'w\\') as fp:\\n      json.dump(avg_scores, fp)\\n    model_itr += 250\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluation multipla"
      ],
      "metadata": {
        "id": "Ui8BYbnWxvlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json "
      ],
      "metadata": {
        "id": "71rFC91-yXnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jjo__l3QdTZW",
        "outputId": "3d436b6c-177e-4a5f-a169-8417316c475b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation multipla\n",
        "\n",
        "\n",
        "\n",
        "model.eval()\n",
        "max_cnt = 250\n",
        "model_itr = 4500\n",
        "max_model_itr = 10000\n",
        "avg_scores = {}\n",
        "with torch.no_grad():\n",
        "\n",
        "  while(model_itr <= max_model_itr):\n",
        "    \n",
        "    if custom:\n",
        "      model.load_state_dict( torch.load( \"/content/drive/MyDrive/AML_Project_2021/Modelli/custom\"+str(model_itr)+\".pth\")) #if on CPU, add this: CPU map_location=torch.device('cpu')\n",
        "    else:\n",
        "      model.load_state_dict( torch.load( \"/content/drive/MyDrive/AML_Project_2021/Modelli/base\"+str(model_itr)+\".pth\")) #if on CPU, add this: CPU map_location=torch.device('cpu')\n",
        "    \n",
        "\n",
        "    #Create df\n",
        "    new_df = pd.DataFrame(columns=[\"boxes\", \"labels\", \"scores\", \"target_boxes\", \"target_labels\"])\n",
        "\n",
        "    #Iterate\n",
        "    cnt = 0\n",
        "    for images, targets in val_data_loader:\n",
        "\n",
        "\n",
        "      images = list(img.to(device) for img in images)\n",
        "      targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "      boxes = targets[0]['boxes'].cpu().numpy().astype(np.int32)\n",
        "        \n",
        "      \n",
        "      outputs = model(images)\n",
        "      print(\"iteration :\",cnt)\n",
        "      outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "      \n",
        "      new_df = new_df.append({\"boxes\" : outputs[0][\"boxes\"].detach().numpy(), \"labels\" : outputs[0][\"labels\"], \"scores\": outputs[0][\"scores\"].detach().numpy(), #there was [outputs[0][\"boxes\"]]\n",
        "                              \"target_boxes\" : torch.tensor(boxes), \"target_labels\": targets[0][\"labels\"],\"img_idx\" : cnt},  #okkio\n",
        "                            ignore_index=True)\n",
        "\n",
        "      cnt+=1\n",
        "      if flag:\n",
        "        if cnt==max_cnt:\n",
        "          break\n",
        "    results = get_avg_precision_at_iou(new_df[\"target_boxes\"],obtain_dict_of_dict(new_df[[\"boxes\", \"scores\", \"img_idx\"]]), 0.5)\n",
        "    avg_scores[model_itr] = results['avg_prec']\n",
        "    with open('/content/drive/MyDrive/AML_Project_2021/Modelli/acc_val'+str(custom)+'.json', 'w') as fp:\n",
        "      json.dump(avg_scores, fp)\n",
        "    print(\"dizionario acc\",avg_scores )\n",
        "    model_itr += 250\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "nciNpMPqxrWr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
